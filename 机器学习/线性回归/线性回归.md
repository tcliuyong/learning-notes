
# 线性回归
 线性回归被认为是很多机器学习书籍的入门模型，其实这个模型的地位并不轻，毕竟线性回归包含几乎所有机器学习方法的流程，所谓 麻雀虽小，五脏俱全。下面我们就 线性回归模型的一些知识。
 ##理论
 ### 数据形式
 对于线性回归我们面对的原始数据就是每一个维度的特征。
 
1.   原始数据：
  $$\vec{x}=(x^{(1)})...x^{(n)},1)^{T}=(\vec{xw}^{T}, 1)^{T}$$

2. 要求的权重：
    $$\vec{w}=(w^{(1)})...w^{(n)},b)^{T}=(\vec{w}^{T}, b)^{T}$$
	
3.  label数据
    $$\vec{y}=(y^{(1)})...y^{(n)})^{T}=(\vec{y})^{T}$$
	
### 目标函数
$$\sum_{i=1}^{N}(\vec{w} *\vec{x} +b -y_{i})=(\vec{y}-(\vec{x_{1}},\vec{x_{2}}...\vec{x_{N}})^{T} * (\vec{y}(\vec{x_{1}},\vec{x_{2}}...\vec{x_{N}})$$
目标函数的意义非常好理解，目标就是让乘完权重的数据的结果与标签很相近即可。
就是：
$$\sum_{i=1}^{N}\vec{w} *=arg\min(\vec{y}-\vec{x}\vec{w})^{T} (\vec{y}-\vec{x}\vec{w})$$

### 求解过程
看了上面的目标函数和求解目标，那目标就很明确了，就是要求E(w),是的目标函数最小，就能够让我们获得一个比较好的结果。

求最小值十分简单，一般就是求导数，让导数为0，从而获取最小点的值。
$$E(w)=(\vec{y}-\vec{xw})^{T} * (\vec{y}-\vec{xw})$$

$$ \frac{	\partial E(\vec{w})}{\partial \vec{w}}=2\vec{x}^{T}(\vec{x}\vec{w} - \vec{y})=0$$ ==>$$\vec{x}^{T}\vec{x}\vec{w}=\vec{x}^{T}\vec{y}$$

**当xx<sup>T</sup>是满秩**,就有如下结论。
$$ \vec{w}^{*}=(\vec{x}^{T}\vec{x})^{-1} \vec{x}^{T}\vec{y}$$

当我们求出整个w，就能够获得预测的方式。
## 特别声明

当**xx<sup>T</sup>不是满秩**，也就是样本数量小于特征数量，也就是方程可能存在多个解，常见的做法就是引入正则化。

## 逻辑回归
现在我们可以思考一个问题。如何用线性回归做分类呢。好吧，什么是分类问题呢？输入上没有太大的变化，但是输出的时候就应该是输入所对应的类别。
对于问题属性而言，很多维度都是离散的维度呈现，用线性回归很难拟合。这个时候我们就会考虑将输入的特征数据，映射到同一个维度上，最好是连续的空间，然后在一个连续的空间做拟合，这个思路实际上就是我们要说的**逻辑回归**。

### 连续空间的选择

假设我们将所有的数据都映射到同一个空间中，那么用什么样子的函数再将这些正无穷到负无穷的映射到一个区间中呢？

你的第一感觉是什么？
采用一个单位阶跃函数是不是十分容易，大于某个值就是分类1，否则就是分类2，但是很遗憾的是这种函数不满足单调可导，使用梯度的时候会带来很大困难。

这个时候会使用一个对数概率函数来代替阶跃函数。也就是常说的sigmod函数。这个函数的奇妙就在于他能将所有实数区间映射到【0,1】之间.

### 逻辑回归的参数估计
首先我们有n维的特征($x_{1}...x_{n}$)。
每维度的特征有一个权重，这个就是学习的目标($w_{1}...w_{n}$)。
对于分类1：
$$P(Y=1|X)=\pi(x)=\frac{exp(w*x)}{1+exp(w*x)}$$

当然对于分类0来说：
$$P(Y=0	|X)=1-\pi(x)=1-\frac{exp(w*x)}{1+exp(w*x)}$$

那么似然函数[^1]来说就是
$$\prod_{i=1}^{N}[\pi(x_{i})]^{y_{i}}[1-\pi({x_{i})}]^{1-y_{i}}$$

那就很容易的知道**对数似然函数**为：
$$L(w)=\sum_{i=1}^{N}y_{i}log\pi(x_{i})+(1-y_{i})log(1-\pi(x_{i})=\sum_{i=1}^{N}y_{i}log\frac{\pi(x_{i})}{1-\pi(x_{i})}+log(1-\pi(x_{i}))$$

由于$\pi(x)=\frac{exp(wx)}{1+exp(wx)}$,所以
$$L(w)=\sum_{i=1}^{N}[y_{i}(w*x)-\log(1+exp(w*x))]$$

目标就是求$L(w)$的极值，从而得到$\vec{w}$的估计值。
这个时候就经常使用梯度下降或是牛顿法求解

# 多分类中的sigmod
以上考虑都是基于二分类的情况，同样可以推广到对分类的情况，设离散变量含有集合Y={1，2，3，4，5...K},则多分类逻辑回归为。

$$P(Y=k|x)=\frac{exp(w_{k}*x)}{1+\sum_{k=1}^{k=K}(exp(w_{k}*x))}\  \ \ \ k\in\{1,2,3,4...(K-1)\}$$ 

$$P(Y=K|x)=\frac{1}{1+\sum_{k=1}^{k=K}(exp(w_{k}*x)）}$$

多分类中的参数根据也类似二分类的参数估计方法。


[^1]: 提到似然函数就要和概率做对比理解，常说的概率是指给定参数后，预测即将发生的事件的可能性。拿硬币这个例子来说，我们已知一枚均匀硬币的正反面概率分别是0.5，要预测抛两次硬币，硬币都朝上的概率：p(HH | pH = 0.5) = 0.5*0.5 = 0.25。而似然概率正好与这个过程相反，我们关注的量不再是事件的发生概率，而是已知发生了某些事件，我们希望知道参数应该是多少。在我们已经抛了两次硬币，并且知道了结果是两次头朝上，这时候，我希望知道这枚硬币抛出去正面朝上的概率为0.5的概率是多少？正面朝上的概率为0.8的概率是多少？如果我们希望知道正面朝上概率为0.5的概率，这个东西就叫做似然函数，可以说成是对某一个参数的猜想（p=0.5）的概率